{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platform Narratives Analysis Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for analyzing social media platform narratives using the modular components from the `src` package. It follows the same analysis steps as the original `matching.ipynb` but in a more structured and reproducible way.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import components from our package\n",
    "from src.ranking import fastLexRank\n",
    "from src.graph_analysis import build_graph, get_descendants, get_tree_nodes, get_posts_from_tree\n",
    "from src.text_processing import clean_text, detect_post_language, filter_posts_by_language\n",
    "from src.utils import load_json_data, create_id_to_post_map, posts_to_dataframe, extract_anchor_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "First, we'll load the data from a JSON file and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load data from JSON file\n",
    "# Update the path to your data file\n",
    "data_path = '../data/your_data.json'\n",
    "data = load_json_data(data_path)\n",
    "print(f\"Loaded {len(data)} posts from {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean and Preprocess Text\n",
    "\n",
    "Next, we'll clean the post text and filter to English posts only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean post text\n",
    "for post in data:\n",
    "    if 'post' in post:\n",
    "        post['post'] = clean_text(post['post'])\n",
    "\n",
    "# Filter to English posts only\n",
    "english_posts = filter_posts_by_language(data)\n",
    "print(f\"Filtered to {len(english_posts)} English posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Post Graph\n",
    "\n",
    "Now we'll build a directed graph representing the relationships between posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Build the post graph\n",
    "post_graph = build_graph(english_posts)\n",
    "print(f\"Built graph with {post_graph.number_of_nodes()} nodes and {post_graph.number_of_edges()} edges\")\n",
    "\n",
    "# Create a mapping from post IDs to posts\n",
    "id_to_post = create_id_to_post_map(english_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply FastLexRank\n",
    "\n",
    "Next, we'll apply FastLexRank to identify significant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert posts to DataFrame for ranking\n",
    "posts_df = pd.DataFrame(english_posts)\n",
    "\n",
    "# Apply FastLexRank to identify significant content\n",
    "ranked_df = fastLexRank(posts_df)\n",
    "print(\"Applied FastLexRank to identify significant content\")\n",
    "\n",
    "# Display the top 5 most significant posts\n",
    "ranked_df.head(5)[['post', 'ap']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Anchor Posts and Their Trees\n",
    "\n",
    "Now we'll identify anchor posts and extract all posts in their trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract anchor posts (posts with no parent or matched_id=0)\n",
    "# If you have a stats DataFrame with matched_id column:\n",
    "# anchor_ids = extract_anchor_ids(stats_df)\n",
    "\n",
    "# For demonstration, we'll use the top 10 ranked posts as anchors\n",
    "top_posts = ranked_df.head(10)\n",
    "anchor_ids = top_posts['id'].values\n",
    "print(f\"Selected {len(anchor_ids)} anchor posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract all posts in the trees rooted at anchor posts\n",
    "anchor_posts_and_replies = []\n",
    "\n",
    "for post_id in anchor_ids:\n",
    "    descendants = get_descendants(post_graph, post_id)\n",
    "    tree_nodes = {post_id} | descendants\n",
    "    posts = [id_to_post.get(node_id) for node_id in tree_nodes]\n",
    "    anchor_posts_and_replies.extend(posts)\n",
    "\n",
    "# Remove None values\n",
    "anchor_posts_and_replies = [post for post in anchor_posts_and_replies if post]\n",
    "\n",
    "print(f\"Extracted {len(anchor_posts_and_replies)} posts from anchor post trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results\n",
    "\n",
    "Finally, we'll analyze the results of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert anchor posts and replies to DataFrame\n",
    "anchor_df = pd.DataFrame(anchor_posts_and_replies)\n",
    "\n",
    "# Apply FastLexRank to the anchor posts and replies\n",
    "ranked_anchor_df = fastLexRank(anchor_df)\n",
    "\n",
    "# Display the top 5 most significant posts in the anchor trees\n",
    "ranked_anchor_df.head(5)[['post', 'ap']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the complete workflow for analyzing social media platform narratives:\n",
    "\n",
    "1. Loading and preparing data\n",
    "2. Cleaning and preprocessing text\n",
    "3. Building a post graph\n",
    "4. Applying FastLexRank to identify significant content\n",
    "5. Extracting anchor posts and their trees\n",
    "6. Analyzing the results\n",
    "\n",
    "This structured approach makes it easy to reproduce the analysis with different datasets and parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 }
}